{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Me  \n",
    "**Michael duPont - CodeCamp 2017**\n",
    "\n",
    "---\n",
    "\n",
    "## Find Faces\n",
    "\n",
    "The first thing we need to do is pick out faces from a larger image. Because the model for this is not user or case specific, we can use an existing model, load it with OpenCV, and tune the hyperparameters instead of building one from scratch, which we will have to do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "CASCADE = cv2.CascadeClassifier('findme/haar_cc_front_face.xml')\n",
    "\n",
    "def find_faces(img: np.ndarray, sf=1.16, mn=5) -> np.array([[int]]):\n",
    "    \"\"\"Returns a list of bounding boxes for every face found in an image\"\"\"\n",
    "    return CASCADE.detectMultiScale(\n",
    "        cv2.cvtColor(img, cv2.COLOR_RGB2GRAY),\n",
    "        scaleFactor=sf,\n",
    "        minNeighbors=mn,\n",
    "        minSize=(45, 45),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's really all we need. Now let's test it by drawing rectangles around a few images of groups. Here's one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(imread('test_imgs/initial/group0.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def draw_boxes(bboxes: [[int]], img: 'np.array', line_width: int=2) -> 'np.array':\n",
    "    \"\"\"Returns an image array with the bounding boxes drawn around potential faces\"\"\"\n",
    "    for x, y, w, h in bboxes:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), line_width)\n",
    "    return img\n",
    "\n",
    "#Find faces for each test image\n",
    "for fname in glob('test_imgs/initial/group*.jpg'):\n",
    "    img = imread(fname)\n",
    "    bboxes = find_faces(img)\n",
    "    print(bboxes)\n",
    "    imsave(fname.replace('initial', 'find_faces'), draw_boxes(bboxes, img))\n",
    "\n",
    "plt.imshow(imread('test_imgs/find_faces/group0.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters, we're getting good face identification over our test images.\n",
    "\n",
    "## Build Dataset\n",
    "\n",
    "### Base Corpus\n",
    "\n",
    "Now let's use this to build a base corpus of \"these faces are not mine\" so we can augment it later with the face we want to target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates cropped faces for imgs matching 'test_imgs/group*.jpg'\n",
    "\n",
    "def crop(img: np.ndarray, x: int, y: int, width: int, height: int) -> np.ndarray:\n",
    "    \"\"\"Returns an image cropped to a given bounding box of top-left coords, width, and height\"\"\"\n",
    "    return img[y:y+height, x:x+width]\n",
    "\n",
    "def pull_faces(glob_in: str, path_out: str) -> int:\n",
    "    \"\"\"Pulls faces out of images found in glob_in and saves them as path_out\n",
    "    Returns the total number of faces found\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for fname in glob(glob_in):\n",
    "        print(fname)\n",
    "        img = imread(fname)\n",
    "        bboxes = find_faces(img)\n",
    "        for bbox in bboxes:\n",
    "            cropped = crop(img, *bbox)\n",
    "            imsave(path_out.format(i), cropped)\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "found = pull_faces('test_imgs/initial/group*.jpg', 'test_imgs/corpus/face{}.jpg')\n",
    "\n",
    "print('Total number of base corpus faces found:', found)\n",
    "plt.imshow(imread('test_imgs/corpus/face0.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some faces to work with, let's save them to a pickle file for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "#Creates base_corpus.pkl from face imgs in test_imgs/corpus\n",
    "imgs = [imread(fname) for fname in glob('test_imgs/corpus/face*.jpg')]\n",
    "dump(imgs, open('findme/base_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Corpus\n",
    "\n",
    "Now we need to add our target data. Since this is going to power a personal project, I'm going to train it to recognize my face. Other than adding some new images, we can reuse the code from before but just supplying a different glob string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found = pull_faces('test_imgs/initial/me*.jpg', 'test_imgs/corpus/me{}.jpg')\n",
    "\n",
    "print('Total number of target faces found:', found)\n",
    "plt.imshow(imread('test_imgs/corpus/me0.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy enough. In order to have a large enough corpus of target faces, I included pictures of myself with other people and deleted their faces after the code block ran. It ended up having eleven target faces.\n",
    "\n",
    "### Model Training Data\n",
    "\n",
    "Now that we have our faces, we need to create the features and labels that will be used to train our facial recognition model. We've already classified our data based on the face's filename; all we need to do is assign a 1 or 0 to each group for our labels. We'll also need to scale each image to a standard size. Thankfully the output for each bounding box is a square, so we don't have to worry about introducing distortions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the two sets of images\n",
    "from pickle import load\n",
    "\n",
    "notme = load(open('findme/base_corpus.pkl', 'rb'))\n",
    "me = [imread(fname) for fname in glob('test_imgs/corpus/me*.jpg')]\n",
    "\n",
    "#Create features and labels\n",
    "features = notme + me\n",
    "labels = [0] * len(notme) + [1] * len(me)\n",
    "\n",
    "#Preprocess images for the model\n",
    "def preprocess(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Resizes a given image and remove alpha channel\"\"\"\n",
    "    img = cv2.resize(img, (45, 45), interpolation=cv2.INTER_AREA)[:,:,:3]\n",
    "    return img\n",
    "\n",
    "features = [preprocess(face) for face in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Simple enough. Let's do a quick check before shuffling. The first image should be part of the base corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Is the target:', labels[0] == 1)\n",
    "plt.imshow(features[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last image should be of the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Is the target:', labels[-1] == 1)\n",
    "plt.imshow(features[-1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Let's create a quick data and file checkpoint. This means we'll be able to load the file in from this point on without having to run most of the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert into numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "dump(features, open('test_imgs/features.pkl', 'wb'))\n",
    "dump(labels, open('test_imgs/labels.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA/FILE CHECKPOINT\n",
    "\n",
    "The notebook can be run from scratch from this point onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA/FILE CHECKPOINT\n",
    "from pickle import load\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "%matplotlib inline\n",
    "from findme.imageutil import crop, draw_boxes, preprocess\n",
    "from findme.models import find_faces\n",
    "\n",
    "features = load(open('findme/features.pkl', 'rb'))\n",
    "labels = load(open('findme/labels.pkl', 'rb'))\n",
    "\n",
    "features = features[-24:]\n",
    "labels = labels[-24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for our data. You'll notice that we only loaded a subset of our dataset. This ensures that the number of target and non-target images matches, which leads to a better model even though it has less data overall. We'll split our data in the next section.\n",
    "\n",
    "# Am I in This?\n",
    "\n",
    "We've already created all of our data. Now for the model we're going to train. First, we need to convert our labels to one-hot encoding for use in the model. This means our output layer will have two nodes: True and False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "labels = enc.fit_transform(labels.reshape(-1, 1)).toarray()\n",
    "print('Not target label:', labels[0])\n",
    "print('Is target label:', labels[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define our model architecture one layer at a time. We'll create three convolutional layers, two fully-connected layers, and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Convolution2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.models import Sequential\n",
    "\n",
    "SHAPE = features[0].shape\n",
    "NB_FILTER = 16\n",
    "\n",
    "def make_model() -> Sequential:\n",
    "    \"\"\"Create a Sequential Keras model to boolean classify faces\"\"\"\n",
    "    model = Sequential()\n",
    "    #First Convolution\n",
    "    model.add(Convolution2D(NB_FILTER, (3, 3), input_shape=SHAPE))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.1))\n",
    "    # Second Convolution\n",
    "    model.add(Convolution2D(NB_FILTER*2, (2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third Convolution\n",
    "    model.add(Convolution2D(NB_FILTER*4, (2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.3))\n",
    "    # Flatten for Fully Connected\n",
    "    model.add(Flatten())\n",
    "    # First Fully Connected\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    # Second Fully Connected\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # Output\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'rmsprop', metrics=[binary_accuracy])\n",
    "    return model\n",
    "\n",
    "print(make_model().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the model. Even though we have a large model in terms of its parameters, we can still let the model train for many epochs because our feature set is so small. On a MacBook Air, it takes around 30 seconds to train the model with 500 epochs. To save space, I've disabled the full training printout that Keras provides, but you can watch the accuracy progress yourself by changing `verbose` from `0` to `1`.\n",
    "\n",
    "We also need to shuffle our data because feeding all of the non-target and target faces into the model in order will lead to a biased model. Scikit-Learn has a convenient function to do this for us. Rather than just calling random, this function preserves the relationship between the feature and label indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "model = KerasClassifier(build_fn=make_model, epochs=500, batch_size=len(labels), verbose=0)\n",
    "X, Y = shuffle(features, labels, random_state=42)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly see how well it trained to the given data. Because the dataset is so small, we didn't want to keep any for a test or validation set. We'll test it on a new image later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(features)\n",
    "print('Non-target faces predicted correctly:', np.all(preds[:12] == 0))\n",
    "print('Non-target faces predicted correctly:', preds[-12:] == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. While Keras has its own mechanisms for training and validating models, we're using a wrapper around our Keras model so it conforms to the Scikit-Learn model API. We can use `fit` and `predict` when working with the model in our code, and it let's us train and use our model with the other helper modules sk-learn provides. For example, we could have evaluated the model using StratifiedKFold and cross_val_score which would look like this:\n",
    "\n",
    "```python\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "model = KerasClassifier(build_fn=make_model, epochs=5, batch_size=len(labels), verbose=0)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "result = cross_val_score(model, features, labels, cv=kfold)\n",
    "print(result.mean())\n",
    "```\n",
    "\n",
    "This method allows us to determine how effective our model is but does not return a trained model for us to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together\n",
    "\n",
    "Lastly, let's create a single function that takes in an image and returns if the target was found and where.\n",
    "\n",
    "First we'll load in our test image. Keep in mind that the model we just trained has never seen this image before and it contains multiple people (and a manatee statue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_img = imread('test_imgs/evaluate/me1.jpg')\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the function itself. Because we've already made function around the core parts of our data pipeline, this function is going to be incredibly short yet powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_in_img(img: np.ndarray) -> (bool, np.array([int])):\n",
    "    \"\"\"Returns whether the target is in a given image and where\"\"\"\n",
    "    for bbox in find_faces(img):\n",
    "        face = preprocess(crop(img, *bbox))\n",
    "        if model.predict(np.array([face])) == 1:\n",
    "            return True, bbox\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah. That's it. Let's break down the steps:\n",
    "\n",
    "- `find_faces` returns a list of bounding boxes containing faces\n",
    "- We prepare each face by cropping the image to the bounding box, scaling to 45x45, and removing the alpha channel\n",
    "- The `model` predicts whether the face is or is not the target\n",
    "- If the target is found (`pred == 1`), return True and the current bounding box\n",
    "- If there aren't any faces or none of the faces belongs to the target, return False and None\n",
    "\n",
    "Now let's test it. If it works properly, we should see a bounding bx appear around the target's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found, bbox = target_in_img(test_img)\n",
    "\n",
    "print('Target face found in test image:', found)\n",
    "if found:\n",
    "    plt.imshow(draw_boxes([bbox], test_img, line_width=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
